{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "17ff0d85-15d2-424a-8268-3ad4fe598a8c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Import the necessary libraries\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.functions import regexp_replace\n",
    "\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "246072c3-bad4-4790-9fad-361e42e95fc5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create a Spark session and set the configurations to access the AWS S3 bucket\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Divvy Bikes - Data Cleaning Using Spark (From AWS S3 Bucket)\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:3.3.4,com.amazonaws:aws-java-sdk-bundle:1.12.481\") \\\n",
    "    .config(\"fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "    .config(\"fs.s3a.metrics.enabled\", \"false\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.aws.credentials.provider\", \"com.amazonaws.auth.DefaultAWSCredentialsProviderChain\") \\\n",
    "    .config(\"spark.driver.memory\", \"8g\") \\\n",
    "    .config(\"spark.executor.memory\", \"8g\") \\\n",
    "    .config(\"spark.memory.fraction\", \"0.8\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7e08ef23-0dd6-4980-a273-1caef00565b9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Part 1. Get the data from AWS S3 Bucket and create a Spark DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "205ec93c-2019-4c66-bc19-182f5697375e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data extracted from S3 is saved to a local Parquet file\n"
     ]
    }
   ],
   "source": [
    "# S3 bucket name\n",
    "bucket_name = \"divvydatacsv\"\n",
    "\n",
    "# Function to read CSV from S3\n",
    "def read_csv_from_s3(file_name):\n",
    "    s3_path = f\"s3a://{bucket_name}/{file_name}\"\n",
    "    return spark.read.csv(s3_path, header=True, inferSchema=True)\n",
    "\n",
    "# Path for local Parquet file\n",
    "local_path = \"local_data/divvy_tripdata_2022full.parquet\"\n",
    "\n",
    "# Check if local Parquet file exists\n",
    "if not os.path.exists(local_path):\n",
    "    # If not, read from S3 and save it as a local Parquet file\n",
    "    months = [\"01\", \"02\", \"03\", \"04\", \"05\", \"06\", \"07\", \"08\", \"09\", \"10\", \"11\", \"12\"]\n",
    "    dataframes = [read_csv_from_s3(f\"divvy_tripdata_2022{month}.csv\") for month in months]\n",
    "    sdf = dataframes[0]\n",
    "    for df in dataframes[1:]:\n",
    "        sdf = sdf.union(df)\n",
    "    \n",
    "    # Save as Parquet file for faster reading in the future\n",
    "    sdf.write.parquet(local_path)\n",
    "    print(\"Data extracted from S3 is saved to a local Parquet file\")\n",
    "else:\n",
    "    print(\"Local Parquet file already exists\")\n",
    "\n",
    "# Read the local Parquet file\n",
    "sdf = spark.read.parquet(local_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "723ae365-288b-43b4-afba-31db94088726",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Part 2. Preview the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "84389593-cc23-47a0-8eaa-bea99e045282",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- ride_id: string (nullable = true)\n |-- rideable_type: string (nullable = true)\n |-- started_at: timestamp (nullable = true)\n |-- ended_at: timestamp (nullable = true)\n |-- start_station_name: string (nullable = true)\n |-- start_station_id: string (nullable = true)\n |-- end_station_name: string (nullable = true)\n |-- end_station_id: string (nullable = true)\n |-- start_lat: double (nullable = true)\n |-- start_lng: double (nullable = true)\n |-- end_lat: double (nullable = true)\n |-- end_lng: double (nullable = true)\n |-- member_casual: string (nullable = true)\n\n5667717\n+----------------+-------------+-------------------+-------------------+--------------------+----------------+--------------------+--------------+------------------+------------------+------------------+------------------+-------------+\n|         ride_id|rideable_type|         started_at|           ended_at|  start_station_name|start_station_id|    end_station_name|end_station_id|         start_lat|         start_lng|           end_lat|           end_lng|member_casual|\n+----------------+-------------+-------------------+-------------------+--------------------+----------------+--------------------+--------------+------------------+------------------+------------------+------------------+-------------+\n|8FAD7F36AB85F1A4| classic_bike|2022-07-18 12:57:40|2022-07-18 13:07:21|  Clark St & Lake St|    KA1503000012|Jefferson St & Mo...|        WL-011|    41.88602082773|    -87.6308760584|41.880329633634624|-87.64274597167967|       member|\n|9196D4D41AD0DA99| classic_bike|2022-07-03 20:09:43|2022-07-03 20:22:31|Ritchie Ct & Bank...|    KA1504000134|Wells St & Hubbar...|  TA1307000151|         41.906866|        -87.626217|         41.889906|        -87.634266|       member|\n|611B509077190B32|electric_bike|2022-07-29 09:01:58|2022-07-29 09:13:56|Ritchie Ct & Bank...|    KA1504000134|Wells St & Hubbar...|  TA1307000151| 41.90676416666667|-87.62606283333334|         41.889906|        -87.634266|       casual|\n|41739A25E84D6C62|electric_bike|2022-07-21 19:04:53|2022-07-21 19:19:59|Ritchie Ct & Bank...|    KA1504000134|Wells St & Hubbar...|  TA1307000151|         41.906876|        -87.626043|         41.889906|        -87.634266|       casual|\n|0E2579FB554B5F42|electric_bike|2022-07-31 05:06:05|2022-07-31 05:12:51|Ritchie Ct & Bank...|    KA1504000134|Wells St & Hubbar...|  TA1307000151|41.906840333333335|       -87.6260075|         41.889906|        -87.634266|       casual|\n+----------------+-------------+-------------------+-------------------+--------------------+----------------+--------------------+--------------+------------------+------------------+------------------+------------------+-------------+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "# Print the schema\n",
    "sdf.printSchema()\n",
    "\n",
    "# Print the number of records\n",
    "print(sdf.count())\n",
    "\n",
    "# Show the first 5 records\n",
    "sdf.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a491cf1a-3c7e-4ae6-acba-500625580329",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Part 3. General Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d6bb8781-06b4-4ca4-b133-cd144d648e2a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows with leading/trailing whitespace in the DataFrame:\n+----------------+-------------+-------------------+-------------------+---------------------------------------+----------------+----------------+--------------+---------+---------+-------+-------+-------------+----------------------+----------------------------+---------------------------------+-------------------------------+-------------------------------+-----------------------------+----------------------------+\n|ride_id         |rideable_type|started_at         |ended_at           |start_station_name                     |start_station_id|end_station_name|end_station_id|start_lat|start_lng|end_lat|end_lng|member_casual|ride_id_has_whitespace|rideable_type_has_whitespace|start_station_name_has_whitespace|start_station_id_has_whitespace|end_station_name_has_whitespace|end_station_id_has_whitespace|member_casual_has_whitespace|\n+----------------+-------------+-------------------+-------------------+---------------------------------------+----------------+----------------+--------------+---------+---------+-------+-------+-------------+----------------------+----------------------------+---------------------------------+-------------------------------+-------------------------------+-----------------------------+----------------------------+\n|CB8D70B54C0DA515|electric_bike|2022-07-03 05:28:24|2022-07-03 05:43:25|Public Rack - Rockwell Ave & Touhy Ave |480             |NULL            |NULL          |42.01    |-87.7    |42.03  |-87.68 |casual       |false                 |false                       |true                             |false                          |false                          |false                        |false                       |\n|23208DDFC29ECF17|electric_bike|2022-07-12 17:51:17|2022-07-12 18:02:58|Public Rack - Rockwell Ave & Touhy Ave |480             |NULL            |NULL          |42.01    |-87.7    |41.98  |-87.7  |casual       |false                 |false                       |true                             |false                          |false                          |false                        |false                       |\n|6E778CA1E92DE5BF|electric_bike|2022-07-13 13:05:08|2022-07-13 13:12:19|Public Rack - Rockwell Ave & Touhy Ave |480             |NULL            |NULL          |42.01    |-87.7    |42.01  |-87.69 |casual       |false                 |false                       |true                             |false                          |false                          |false                        |false                       |\n|133B586CF70589A1|electric_bike|2022-07-26 15:56:02|2022-07-26 16:02:36|Public Rack - Rockwell Ave & Touhy Ave |480             |NULL            |NULL          |42.01    |-87.7    |42.0   |-87.7  |casual       |false                 |false                       |true                             |false                          |false                          |false                        |false                       |\n|588758C92896F8F1|electric_bike|2022-07-14 21:16:07|2022-07-14 21:23:58|Public Rack - Rockwell Ave & Touhy Ave |480             |NULL            |NULL          |42.01    |-87.7    |42.01  |-87.67 |member       |false                 |false                       |true                             |false                          |false                          |false                        |false                       |\n|E332A4BE91495E5E|electric_bike|2022-07-28 13:34:53|2022-07-28 13:38:46|Public Rack - Rockwell Ave & Touhy Ave |480             |NULL            |NULL          |42.01    |-87.7    |42.01  |-87.71 |member       |false                 |false                       |true                             |false                          |false                          |false                        |false                       |\n+----------------+-------------+-------------------+-------------------+---------------------------------------+----------------+----------------+--------------+---------+---------+-------+-------+-------------+----------------------+----------------------------+---------------------------------+-------------------------------+-------------------------------+-----------------------------+----------------------------+\nonly showing top 6 rows\n\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 31:================================>                        (9 + 7) / 16]\r"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows with whitespace in the DataFrame: 328\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Let's begin by checking trailing and leading whitespace\n",
    "def has_whitespace(col):\n",
    "    return (F.length(col) != F.length(F.trim(col)))\n",
    "\n",
    "whitespace_check = [F.when(has_whitespace(F.col(c)), F.lit(True)).otherwise(F.lit(False)).alias(f\"{c}_has_whitespace\") \n",
    "                    for c in sdf.columns if sdf.schema[c].dataType == StringType()]\n",
    "\n",
    "sdf_with_check = sdf.select(\"*\", *whitespace_check)\n",
    "rows_with_whitespace = sdf_with_check.filter(F.array_contains(F.array(*[f\"{c}_has_whitespace\" for c in sdf.columns if sdf.schema[c].dataType == StringType()]), True))\n",
    "\n",
    "print(\"Rows with leading/trailing whitespace in the DataFrame:\")\n",
    "rows_with_whitespace.show(6, truncate=False)\n",
    "\n",
    "# Count the rows with whitespace\n",
    "original_count = rows_with_whitespace.count()\n",
    "\n",
    "print(f\"Number of rows with whitespace in the DataFrame: {original_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7d257656-edcc-490b-9021-cc3c4fb406e3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Now we know that there are multiple rows with whitespace, we need to use trim() to fix this\n",
    "sdf = sdf.select([F.trim(F.col(c)).alias(c) if sdf.schema[c].dataType == StringType() else F.col(c) for c in sdf.columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "10a4b67a-5984-4309-9be7-180c6e681dcf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows with leading/trailing whitespace in the DataFrame:\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------+----------+--------+------------------+----------------+----------------+--------------+---------+---------+-------+-------+-------------+----------------------+----------------------------+---------------------------------+-------------------------------+-------------------------------+-----------------------------+----------------------------+\n|ride_id|rideable_type|started_at|ended_at|start_station_name|start_station_id|end_station_name|end_station_id|start_lat|start_lng|end_lat|end_lng|member_casual|ride_id_has_whitespace|rideable_type_has_whitespace|start_station_name_has_whitespace|start_station_id_has_whitespace|end_station_name_has_whitespace|end_station_id_has_whitespace|member_casual_has_whitespace|\n+-------+-------------+----------+--------+------------------+----------------+----------------+--------------+---------+---------+-------+-------+-------------+----------------------+----------------------------+---------------------------------+-------------------------------+-------------------------------+-----------------------------+----------------------------+\n+-------+-------------+----------+--------+------------------+----------------+----------------+--------------+---------+---------+-------+-------+-------------+----------------------+----------------------------+---------------------------------+-------------------------------+-------------------------------+-----------------------------+----------------------------+\n\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 37:==============>                                         (4 + 12) / 16]\r"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows with whitespace in the DataFrame: 0\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Since we've cleaned the data, and since we've updated 'sdf', let's check the whitespace again\n",
    "sdf_with_check = sdf.select(\"*\", *whitespace_check)\n",
    "rows_with_whitespace = sdf_with_check.filter(F.array_contains(F.array(*[f\"{c}_has_whitespace\" for c in sdf.columns if sdf.schema[c].dataType == StringType()]), True))\n",
    "\n",
    "print(\"Rows with leading/trailing whitespace in the DataFrame:\")\n",
    "rows_with_whitespace.show(truncate=False)\n",
    "\n",
    "# Count the rows with whitespace in the original DataFrame\n",
    "original_count = rows_with_whitespace.count()\n",
    "\n",
    "print(f\"Number of rows with whitespace in the DataFrame: {original_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7ad48415-fd8a-4270-aab5-e51e7825d2e5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "For the next data cleaning steps, let's utilize Spark SQL. You can utilize Spark SQL or use standard DataFrame operations (up to you), but I utilize both of them in this notebook for demonstration purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2bbe71a8-72d7-4896-857f-10bc4df308a1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# To utilize Spark SQL, we can start by creating a temporary view\n",
    "sdf.createOrReplaceTempView(\"sdf_view\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c311c254-c03f-4f82-9142-d85b787238eb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 40:==============>                                         (4 + 12) / 16]\r"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------+----------+--------+------------------+----------------+----------------+--------------+---------+---------+-------+-------+-------------+\n|ride_id|rideable_type|started_at|ended_at|start_station_name|start_station_id|end_station_name|end_station_id|start_lat|start_lng|end_lat|end_lng|member_casual|\n+-------+-------------+----------+--------+------------------+----------------+----------------+--------------+---------+---------+-------+-------+-------------+\n|      0|            0|         0|       0|            833064|          833064|          892742|        892742|        0|        0|   5858|   5858|            0|\n+-------+-------------+----------+--------+------------------+----------------+----------------+--------------+---------+---------+-------+-------+-------------+\n\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Count the number of null values in each column\n",
    "null_counts_query = \"SELECT \" + \", \".join([f\"SUM(CASE WHEN {c} IS NULL THEN 1 ELSE 0 END) AS {c}\" for c in sdf.columns]) + \" FROM sdf_view\"\n",
    "null_counts = spark.sql(null_counts_query)\n",
    "null_counts.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0618215d-34ae-4395-ad03-e84c55313150",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 43:===>                                                    (1 + 15) / 16]\r"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4369360\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Now we know that there are null values, let's use Spark SQL to drop rows with any null values\n",
    "sdf = spark.sql(\"SELECT * FROM sdf_view WHERE \" + \" AND \".join([f\"{c} IS NOT NULL\" for c in sdf.columns]))\n",
    "\n",
    "# Show the number of records after dropping rows with null values\n",
    "print(sdf.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4b7467d5-0ade-4fd0-8d8a-128282078fa5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Update the temporary view since we have updated the Spark DataFrame\n",
    "sdf.createOrReplaceTempView(\"sdf_view\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d1b3740b-14a4-4305-976b-e677eb002fcd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 46:============================>                            (8 + 8) / 16]\r"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+-----------+\n|         started_at|           ended_at|ride_length|\n+-------------------+-------------------+-----------+\n|2022-10-13 14:42:10|2022-10-13 11:53:28|     -10122|\n|2022-06-07 19:14:47|2022-06-07 17:05:42|      -7745|\n|2022-06-07 19:14:46|2022-06-07 17:07:45|      -7621|\n|2022-11-06 01:58:11|2022-11-06 01:00:12|      -3479|\n|2022-11-06 01:59:05|2022-11-06 01:02:03|      -3422|\n|2022-11-06 01:57:21|2022-11-06 01:02:07|      -3314|\n+-------------------+-------------------+-----------+\nonly showing top 6 rows\n\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# According to Divvy Data website, trips below 60 seconds in length should be removed. \n",
    "# Therefore, let's create \"ride_length\" based on \"started_at\" and \"ended_at\" to calculate and investigate this\n",
    "spark.sql(\"\"\"\n",
    "    SELECT *\n",
    "    FROM (\n",
    "        SELECT started_at, ended_at, (unix_timestamp(ended_at) - unix_timestamp(started_at)) AS ride_length \n",
    "        FROM sdf_view\n",
    "    ) subquery\n",
    "    WHERE ride_length < 60\n",
    "    ORDER BY ride_length\n",
    "\"\"\").show(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0f0cf2fe-8901-4f18-a583-b7e566f1df7d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 47:===>                                                    (1 + 15) / 16]\r"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-------------+-------------------+-------------------+--------------------+----------------+--------------------+--------------+-----------------+------------------+-----------------+------------------+-------------+-----------+\n|         ride_id|rideable_type|         started_at|           ended_at|  start_station_name|start_station_id|    end_station_name|end_station_id|        start_lat|         start_lng|          end_lat|           end_lng|member_casual|ride_length|\n+----------------+-------------+-------------------+-------------------+--------------------+----------------+--------------------+--------------+-----------------+------------------+-----------------+------------------+-------------+-----------+\n|3CC6543FEE9EF143| classic_bike|2022-12-01 06:34:52|2022-12-01 06:35:52|Indiana Ave & Roo...|          SL-005|Indiana Ave & Roo...|        SL-005|        41.867888|        -87.623041|        41.867888|        -87.623041|       member|         60|\n|167C3FC123F87456| classic_bike|2022-09-07 08:49:33|2022-09-07 08:50:33|Fairbanks Ct & Gr...|    TA1305000003|Fairbanks Ct & Gr...|  TA1305000003|41.89184737210993|-87.62058019638062|41.89184737210993|-87.62058019638062|       member|         60|\n|D607FFD5DCF8CFB2|electric_bike|2022-07-20 08:25:02|2022-07-20 08:26:02|  May St & Taylor St|           13160|  May St & Taylor St|         13160|        41.869426|       -87.6554335|       41.8694821|       -87.6554864|       member|         60|\n|D78FF53606F7BCE8|electric_bike|2022-09-24 12:51:53|2022-09-24 12:52:53|Elston Ave & Geor...|             472|Elston Ave & Geor...|           472|            41.93|            -87.69|            41.93|            -87.69|       member|         60|\n|3909D68B9E136933|electric_bike|2022-08-17 15:58:42|2022-08-17 15:59:42|DuSable Lake Shor...|    TA1309000049|DuSable Lake Shor...|  TA1309000049|     41.940779567|     -87.639207244|        41.940775|        -87.639192|       member|         60|\n|402EC73917248932| classic_bike|2022-09-29 16:55:02|2022-09-29 16:56:02| Perry Ave & 69th St|    KA1503000047| Perry Ave & 69th St|  KA1503000047|   41.76929308995|    -87.6281959291|   41.76929308995|    -87.6281959291|       casual|         60|\n+----------------+-------------+-------------------+-------------------+--------------------+----------------+--------------------+--------------+-----------------+------------------+-----------------+------------------+-------------+-----------+\nonly showing top 6 rows\n\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Now we know that there are ride_length with less than 60 seconds (they even have minus numbers), it's time to clean them up\n",
    "sdf = spark.sql(\"\"\"\n",
    "        SELECT *\n",
    "        FROM (\n",
    "            SELECT *, (unix_timestamp(ended_at) - unix_timestamp(started_at)) AS ride_length \n",
    "            FROM sdf_view\n",
    "        ) subquery\n",
    "        WHERE ride_length >= 60\n",
    "        ORDER BY ride_length\n",
    "    \"\"\")\n",
    "\n",
    "sdf.show(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c55836f6-f174-4d5a-90eb-c4aa6670a61a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Update the temporary view since we have updated the Spark DataFrame\n",
    "sdf.createOrReplaceTempView(\"sdf_view\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "80c53d5b-4291-4740-9b83-70446ae0ebb7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Part 4. Investigate Data Quality Issues (before more specific data cleaning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "039e5c4c-1a0c-49c3-b245-414745dfc972",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 48:=================>                                      (5 + 11) / 16]\r"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-------------+-------------------+-------------------+------------------+--------------------+--------------------+--------------+------------------+------------------+------------------+------------------+-------------+-----------+\n|         ride_id|rideable_type|         started_at|           ended_at|start_station_name|    start_station_id|    end_station_name|end_station_id|         start_lat|         start_lng|           end_lat|           end_lng|member_casual|ride_length|\n+----------------+-------------+-------------------+-------------------+------------------+--------------------+--------------------+--------------+------------------+------------------+------------------+------------------+-------------+-----------+\n|226015676A82060F|electric_bike|2022-06-22 15:08:17|2022-06-22 15:25:14|   WEST CHI-WATSON|DIVVY 001 - Wareh...|Milwaukee Ave & F...|           428|         41.924738|-87.70064133333334|             41.92|             -87.7|       casual|       1017|\n|0E2BDA1E247E7F0C|electric_bike|2022-08-13 21:06:10|2022-08-13 21:15:15|   WEST CHI-WATSON|DIVVY 001 - Wareh...|  Long & Irving Park|           398|41.927668833333335|-87.76981549999999|             41.95|            -87.76|       casual|        545|\n|599C6337A1F1635A|electric_bike|2022-05-07 19:21:15|2022-05-07 19:29:33|   WEST CHI-WATSON|DIVVY 001 - Wareh...|Wood St & Taylor ...|         13285|        41.8516735|-87.69061833333333|41.869265218438194|-87.67373085021973|       member|        498|\n|A09927982BFB48F1|electric_bike|2022-04-23 18:43:39|2022-04-23 19:10:22|   WEST CHI-WATSON|           DIVVY 001|St Louis Ave & No...|           385|         41.794383|        -87.711208|             41.79|            -87.71|       casual|       1603|\n+----------------+-------------+-------------------+-------------------+------------------+--------------------+--------------------+--------------+------------------+------------------+------------------+------------------+-------------+-----------+\n\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Let's check and investigate potential data quality issues. We can start by checking uppercase values from start_station_name \n",
    "# to find naming inconsistencies.\n",
    "spark.sql(\"\"\"\n",
    "    SELECT *\n",
    "    FROM sdf_view\n",
    "    WHERE UPPER(start_station_name) = start_station_name\n",
    "    ORDER BY start_station_name\n",
    "\"\"\").show(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3932f293-7b64-49fe-a2b5-52c05ee0c245",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "I found something interesting above. Check the rows where start_station_name have the value \"WEST CHI-WATSON\". These same rows show the start_station_id as \"DIVVY\" something. Let's check further if all start_station_id that contain the text \"DIVVY\" are simple test stations or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f9948119-468c-4883-8d59-ec7c7a0d04b0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     start_station_id start_station_name\n0  DIVVY 001 - Warehouse test station    WEST CHI-WATSON\n1  DIVVY 001 - Warehouse test station    WEST CHI-WATSON\n2  DIVVY 001 - Warehouse test station    WEST CHI-WATSON\n3                           DIVVY 001    WEST CHI-WATSON\n4  DIVVY 001 - Warehouse test station            WestChi\n5  DIVVY 001 - Warehouse test station            WestChi\n"
     ]
    }
   ],
   "source": [
    "# let's use pandas so we can display the full station names for the start_station_name\n",
    "result_pd = spark.sql(\"\"\"\n",
    "    SELECT start_station_id, start_station_name\n",
    "    FROM sdf_view\n",
    "    WHERE start_station_id LIKE '%DIVVY%'\n",
    "    ORDER BY start_station_name\n",
    "\"\"\").toPandas()\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "print(result_pd.head(6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "67115308-9578-4b14-b7e3-4fe621943f80",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "From the latest finding above, one row has the value \"DIVVY 001\" for its start_station_id, while the other rows have the value \"DIVVY 001 - Warehouse test station\". While it's likely \"DIVVY 001\" is also a test station, we are not sure, so let's ignore it for now. Let's just make a note to clean up the rows later, where the start_station_id has the word \"test\", but not specifically \"DIVVY\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "62e86f81-4bcb-4379-bfbe-3da234c3f26f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 53:====================================================>   (15 + 1) / 16]\r"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-------------+-------------------+-------------------+--------------------+----------------+--------------------+--------------------+---------+----------+------------------+------------------+-------------+-----------+\n|         ride_id|rideable_type|         started_at|           ended_at|  start_station_name|start_station_id|    end_station_name|      end_station_id|start_lat| start_lng|           end_lat|           end_lng|member_casual|ride_length|\n+----------------+-------------+-------------------+-------------------+--------------------+----------------+--------------------+--------------------+---------+----------+------------------+------------------+-------------+-----------+\n|E5B1B30C08A03D7B| classic_bike|2022-06-04 15:28:21|2022-06-04 15:46:03|   Adler Planetarium|           13431|DIVVY CASSETTE RE...|DIVVY CASSETTE RE...|41.866095|-87.607267|41.895543469489944|-87.60292053222655|       casual|       1062|\n|C6E1B2965BCED6B6| classic_bike|2022-08-02 23:36:06|2022-08-02 23:42:49|Avers Ave & Belmo...|           15640|DIVVY CASSETTE RE...|DIVVY CASSETTE RE...|41.939408|-87.723574|41.895543469489944|-87.60292053222655|       casual|        403|\n|F25FE175AFAB5DD0| classic_bike|2022-08-03 07:36:02|2022-08-03 07:42:49| Canal St & Adams St|           13011|DIVVY CASSETTE RE...|DIVVY CASSETTE RE...|41.879255|-87.639904|41.895543469489944|-87.60292053222655|       member|        407|\n|5441EAF88B22331F| classic_bike|2022-08-03 17:56:34|2022-08-03 18:02:46|DuSable Lake Shor...|    TA1309000049|DIVVY CASSETTE RE...|DIVVY CASSETTE RE...|41.940775|-87.639192|41.895543469489944|-87.60292053222655|       member|        372|\n|104C043BDD8E9184| classic_bike|2022-08-03 07:09:31|2022-08-03 07:14:49|Logan Blvd & Elst...|    TA1308000031|DIVVY CASSETTE RE...|DIVVY CASSETTE RE...|41.929465|-87.684158|41.895543469489944|-87.60292053222655|       member|        318|\n|685D6988F1C63EF6| classic_bike|2022-08-03 17:37:08|2022-08-03 17:40:46|Southport Ave & I...|    TA1309000043|DIVVY CASSETTE RE...|DIVVY CASSETTE RE...|41.954177|-87.664358|41.895543469489944|-87.60292053222655|       casual|        218|\n+----------------+-------------+-------------------+-------------------+--------------------+----------------+--------------------+--------------------+---------+----------+------------------+------------------+-------------+-----------+\n\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Next, let's check the uppercase values from end_station_name\n",
    "spark.sql(\"\"\"\n",
    "    SELECT *\n",
    "    FROM sdf_view\n",
    "    WHERE UPPER(end_station_name) = end_station_name\n",
    "    ORDER BY start_station_name\n",
    "\"\"\").show(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "adcb6133-9d9f-4eab-a872-1357fa34b6a3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Another interesting finding here. The above result shows that for the rows that contain the text \"DIVVY\" in their end_station_name, they also start with the text \"DIVVY\" for their end_station_id. Let's investigate this further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9380b138-cfb8-4dff-9593-db1e75c7e471",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         end_station_id                      end_station_name\n0  DIVVY CASSETTE REPAIR MOBILE STATION  DIVVY CASSETTE REPAIR MOBILE STATION\n1  DIVVY CASSETTE REPAIR MOBILE STATION  DIVVY CASSETTE REPAIR MOBILE STATION\n2  DIVVY CASSETTE REPAIR MOBILE STATION  DIVVY CASSETTE REPAIR MOBILE STATION\n3  DIVVY CASSETTE REPAIR MOBILE STATION  DIVVY CASSETTE REPAIR MOBILE STATION\n4  DIVVY CASSETTE REPAIR MOBILE STATION  DIVVY CASSETTE REPAIR MOBILE STATION\n5  DIVVY CASSETTE REPAIR MOBILE STATION  DIVVY CASSETTE REPAIR MOBILE STATION\n"
     ]
    }
   ],
   "source": [
    "# Just like before, let's use pandas so we can display the full station names (but this time for the end_station_name)\n",
    "result_pd = spark.sql(\"\"\"\n",
    "    SELECT end_station_id, end_station_name\n",
    "    FROM sdf_view\n",
    "    WHERE end_station_id LIKE '%DIVVY%' OR end_station_name LIKE '%DIVVY%'\n",
    "    ORDER BY end_station_name\n",
    "\"\"\").toPandas()\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "print(result_pd.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b871b360-fc57-410b-a38a-82678ec928c5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "There are rows with value \"DIVVY CASSETTE REPAIR MOBILE STATION\", which means they are just used for maintenance, not for actual trips. This needs to be filtered as well later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "176c477c-5498-43b9-9d35-e9072e3792d8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------+----------+--------+------------------+----------------+----------------+--------------+---------+---------+-------+-------+-------------+-----------+\n|ride_id|rideable_type|started_at|ended_at|start_station_name|start_station_id|end_station_name|end_station_id|start_lat|start_lng|end_lat|end_lng|member_casual|ride_length|\n+-------+-------------+----------+--------+------------------+----------------+----------------+--------------+---------+---------+-------+-------+-------------+-----------+\n+-------+-------------+----------+--------+------------------+----------------+----------------+--------------+---------+---------+-------+-------+-------------+-----------+\n\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Now, let's check the lowercase values from start_station_name\n",
    "spark.sql(\"\"\"\n",
    "    SELECT *\n",
    "    FROM sdf_view\n",
    "    WHERE LOWER(start_station_name) = start_station_name\n",
    "    ORDER BY start_station_name\n",
    "\"\"\").show(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "342c94c3-1bd6-4914-b0fc-10d6497208d0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "There's nothing here, let's continue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "286ab3ae-6194-47d7-b2a3-51d8e25e6f02",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------+----------+--------+------------------+----------------+----------------+--------------+---------+---------+-------+-------+-------------+-----------+\n|ride_id|rideable_type|started_at|ended_at|start_station_name|start_station_id|end_station_name|end_station_id|start_lat|start_lng|end_lat|end_lng|member_casual|ride_length|\n+-------+-------------+----------+--------+------------------+----------------+----------------+--------------+---------+---------+-------+-------+-------------+-----------+\n+-------+-------------+----------+--------+------------------+----------------+----------------+--------------+---------+---------+-------+-------+-------------+-----------+\n\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Let's check the lowercase values from end_station_name\n",
    "spark.sql(\"\"\"\n",
    "    SELECT *\n",
    "    FROM sdf_view\n",
    "    WHERE LOWER(end_station_name) = end_station_name\n",
    "    ORDER BY end_station_name\n",
    "\"\"\").show(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "59274575-69c9-4c0a-b51b-0491b91c2728",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Nothing to see here as well. Let's continue. \n",
    "\n",
    "Now that we know some rows have the word \"test\" in start_station_id (from one of the previous investigations), we should check all the station columns for the word \"test\". Let's check regardless of their case sensitivity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8600300d-96ea-4857-bb67-60efb7f738d3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 61:==========================================>             (12 + 4) / 16]\r"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      start_station_id     start_station_name end_station_id  \\\n0  Hubbard Bike-checking (LBS-WH-TEST)  Base - 2132 W Hubbard   TA1307000138   \n1  Hubbard Bike-checking (LBS-WH-TEST)  Base - 2132 W Hubbard          13285   \n2   DIVVY 001 - Warehouse test station        WEST CHI-WATSON          13285   \n3  Hubbard Bike-checking (LBS-WH-TEST)  Base - 2132 W Hubbard          13221   \n4  Hubbard Bike-checking (LBS-WH-TEST)  Base - 2132 W Hubbard          13221   \n5  Hubbard Bike-checking (LBS-WH-TEST)  Base - 2132 W Hubbard          13221   \n6  Hubbard Bike-checking (LBS-WH-TEST)  Base - 2132 W Hubbard          13221   \n7  Hubbard Bike-checking (LBS-WH-TEST)  Base - 2132 W Hubbard          13432   \n8  Hubbard Bike-checking (LBS-WH-TEST)  Base - 2132 W Hubbard          13432   \n9  Hubbard Bike-checking (LBS-WH-TEST)  Base - 2132 W Hubbard            637   \n\n             end_station_name  \n0       Wood St & Webster Ave  \n1  Wood St & Taylor St (Temp)  \n2  Wood St & Taylor St (Temp)  \n3     Wood St & Milwaukee Ave  \n4     Wood St & Milwaukee Ave  \n5     Wood St & Milwaukee Ave  \n6     Wood St & Milwaukee Ave  \n7        Wood St & Hubbard St  \n8        Wood St & Hubbard St  \n9       Wood St & Chicago Ave  \n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Again, let's use pandas so we can display the full station names\n",
    "result_pd = spark.sql(\"\"\"\n",
    "    SELECT start_station_id, start_station_name, end_station_id, end_station_name\n",
    "    FROM sdf_view\n",
    "    WHERE LOWER(start_station_name) LIKE '%test%'\n",
    "        OR LOWER(end_station_name) LIKE '%test%'\n",
    "        OR LOWER(start_station_id) LIKE '%test%'\n",
    "        OR LOWER(end_station_id) LIKE '%test%'\n",
    "    ORDER BY end_station_name DESC\n",
    "\"\"\").toPandas()\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "print(result_pd.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "13d04091-e26b-4525-afd2-7d1da31e0e61",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Apart from the issues with the word \"test\", there are also station names that end with \"(Temp)\", which probably means \"Temporary\". We will have to fix the naming inconsistencies here later. Let's investigate this further. Let's check for the word \"temp\" in the station columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ffd81eaf-5291-4744-b280-efbed74d45a9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  start_station_id   start_station_name end_station_id  \\\n0     KA1504000155  2112 W Peterson Ave          13259   \n1            15491        63rd St Beach   TA1308000026   \n2            13028    900 W Harrison St          13285   \n3            13028    900 W Harrison St          13285   \n4            13028    900 W Harrison St          13285   \n5            13028    900 W Harrison St          13285   \n\n                     end_station_name  \n0  California Ave & Francis Pl (Temp)  \n1      Wentworth Ave & 24th St (Temp)  \n2          Wood St & Taylor St (Temp)  \n3          Wood St & Taylor St (Temp)  \n4          Wood St & Taylor St (Temp)  \n5          Wood St & Taylor St (Temp)  \n"
     ]
    }
   ],
   "source": [
    "# Let's use pandas so we can check the full station names\n",
    "result_pd = spark.sql(\"\"\"\n",
    "    SELECT start_station_id, start_station_name, end_station_id, end_station_name\n",
    "    FROM sdf_view\n",
    "    WHERE LOWER(start_station_name) LIKE '%temp%'\n",
    "        OR LOWER(end_station_name) LIKE '%temp%'\n",
    "        OR LOWER(start_station_id) LIKE '%temp%'\n",
    "        OR LOWER(end_station_id) LIKE '%temp%'\n",
    "    ORDER BY start_station_name\n",
    "\"\"\").toPandas()\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "print(result_pd.head(6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d5be2ad8-dc21-40eb-b211-38e15a2bb2e2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "I found multiple station names with \"(Temp)\", we will have to fix the station names later. Another potential data quality issue is the presence of the asterisk character in the station columns. Let's check for the presence of the asterisk character in the station columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5f1b0f62-4461-4eb5-b646-6cec1da17a4a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  start_station_id       start_station_name end_station_id  \\\n0     chargingstx3  Green St & Randolph St*   TA1307000120   \n1     chargingstx3  Green St & Randolph St*   chargingstx3   \n2     chargingstx4     Morgan St & Lake St*        20246.0   \n3     chargingstx3  Green St & Randolph St*          13053   \n4    chargingstx07  Green St & Madison Ave*          13053   \n5     chargingstx3  Green St & Randolph St*          13409   \n\n                end_station_name  \n0          Green St & Madison St  \n1        Green St & Randolph St*  \n2         N Green St & W Lake St  \n3     Green St & Washington Blvd  \n4     Green St & Washington Blvd  \n5  Sangamon St & Washington Blvd  \n"
     ]
    }
   ],
   "source": [
    "# Again, let's use pandas to check the full station names. This time, to check the names that contain asterisks (*)\n",
    "result_pd = spark.sql(\"\"\"\n",
    "    SELECT start_station_id, start_station_name, end_station_id, end_station_name\n",
    "    FROM sdf_view\n",
    "    WHERE LOWER(start_station_name) LIKE '%*%'\n",
    "        OR LOWER(end_station_name) LIKE '%*%'\n",
    "        OR LOWER(start_station_id) LIKE '%*%'\n",
    "        OR LOWER(end_station_id) LIKE '%*%'\n",
    "\"\"\").toPandas()\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "print(result_pd.head(6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d743bfc1-e5e8-4d12-9127-6944c2c10f7c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Many rows have asterisks in the station names, we need to fix them later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9ab9a158-6279-4720-94c6-c157d9ab5d39",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Part 5 - Specific Data Cleaning (to fix the issues from part 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "feafb4f1-3cff-4166-a901-49db67db8bcc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Clean and fix all the data quality issues we have identified from part 4\n",
    "sdf = spark.sql(\"\"\"\n",
    "    SELECT * \n",
    "    FROM sdf_view\n",
    "    WHERE upper(start_station_name) != start_station_name\n",
    "      AND upper(end_station_name) != end_station_name\n",
    "      AND lower(start_station_name) NOT LIKE '%test%'\n",
    "      AND lower(end_station_name) NOT LIKE '%test%'\n",
    "      AND lower(start_station_id) NOT LIKE '%test%'\n",
    "      AND lower(end_station_id) NOT LIKE '%test%'\n",
    "      AND lower(start_station_name) NOT LIKE 'divvy cassette repair mobile station'\n",
    "      AND lower(end_station_name) NOT LIKE 'divvy cassette repair mobile station'\n",
    "      AND lower(start_station_id) NOT LIKE 'divvy cassette repair mobile station'\n",
    "      AND lower(end_station_id) NOT LIKE 'divvy cassette repair mobile station'\n",
    "\"\"\")\n",
    "\n",
    "sdf = sdf.withColumn(\"start_station_name\", regexp_replace(\"start_station_name\", \"\\\\s?\\\\*\", \"\")) \\\n",
    "        .withColumn(\"start_station_name\", regexp_replace(\"start_station_name\", \"\\\\s?\\\\(Temp\\\\)\", \"\")) \\\n",
    "        .withColumn(\"end_station_name\", regexp_replace(\"end_station_name\", \"\\\\s?\\\\*\", \"\")) \\\n",
    "        .withColumn(\"end_station_name\", regexp_replace(\"end_station_name\", \"\\\\s?\\\\(Temp\\\\)\", \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3194ba8a-07bf-4837-9bae-cdde0a12c4d5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 72:====================================================>   (15 + 1) / 16]\r"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-------------+-------------------+-------------------+--------------------+----------------+--------------------+--------------+-----------------+-----------------+-----------------+-----------------+-------------+-----------+\n|         ride_id|rideable_type|         started_at|           ended_at|  start_station_name|start_station_id|    end_station_name|end_station_id|        start_lat|        start_lng|          end_lat|          end_lng|member_casual|ride_length|\n+----------------+-------------+-------------------+-------------------+--------------------+----------------+--------------------+--------------+-----------------+-----------------+-----------------+-----------------+-------------+-----------+\n|86221D7038654BC9| classic_bike|2022-12-08 20:30:09|2022-12-08 20:31:09|Avondale Ave & Ir...|           15624|Avondale Ave & Ir...|         15624|        41.953393|       -87.732002|        41.953393|       -87.732002|       member|         60|\n|C88FF4AA14FA1B8B|electric_bike|2022-07-25 14:00:28|2022-07-25 14:01:28|Streeter Dr & Gra...|           13022|Streeter Dr & Gra...|         13022|      41.89226687|    -87.611988544|        41.892278|       -87.612043|       casual|         60|\n|3F00F262075AA647|electric_bike|2022-09-10 14:44:33|2022-09-10 14:45:33|Sheridan Rd & Mon...|    TA1307000107|Sheridan Rd & Mon...|  TA1307000107|       41.9615205|      -87.6546415|         41.96167|        -87.65464|       casual|         60|\n|023185BB9E13F8A2| classic_bike|2022-07-03 18:50:29|2022-07-03 18:51:29|Broadway & Barry Ave|           13137|Broadway & Barry Ave|         13137|41.93758231600629|-87.6440978050232|41.93758231600629|-87.6440978050232|       casual|         60|\n|1E05AACF7AA7EB1D| classic_bike|2022-07-21 18:41:41|2022-07-21 18:42:41|Ritchie Ct & Bank...|    KA1504000134|Ritchie Ct & Bank...|  KA1504000134|        41.906866|       -87.626217|        41.906866|       -87.626217|       casual|         60|\n+----------------+-------------+-------------------+-------------------+--------------------+----------------+--------------------+--------------+-----------------+-----------------+-----------------+-----------------+-------------+-----------+\nonly showing top 5 rows\n\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Update the temporary view since we have updated the Spark DataFrame\n",
    "sdf.createOrReplaceTempView(\"sdf_view\")\n",
    "\n",
    "# Also, show the cleaned data\n",
    "sdf.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "54254696-b1c5-4922-8f71-f8b82af0c3c0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 74:===================================>                    (10 + 6) / 16]\r"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  start_station_id      start_station_name\n0     chargingstx3  Green St & Randolph St\n1     chargingstx3  Green St & Randolph St\n2     chargingstx3  Green St & Randolph St\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Test 1 - Making sure that the data has been properly cleaned, we should check the new 'sdf_view'. \n",
    "# We know station id 'chargingstx3' previously  had station name of 'Green St & Randolph St*', \n",
    "# so now we should see 'Green St & Randolph St' instead. \n",
    "result_pd = spark.sql(\"\"\"\n",
    "    SELECT start_station_id, start_station_name\n",
    "    FROM sdf_view\n",
    "    WHERE start_station_id = 'chargingstx3'\n",
    "\"\"\").toPandas()\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "print(result_pd.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8ff28300-e424-4b69-a355-72fa76aec9bf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 78:=================>                                      (5 + 11) / 16]\r"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  end_station_id      end_station_name\n0          13285  Wood St & Taylor St \n1          13285  Wood St & Taylor St \n2          13285  Wood St & Taylor St \n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Test 2 - We know station id '13285' previously had station name of 'Wood St & Taylor St (Temp)', \n",
    "# so now we should see 'Wood St & Taylor St' instead.\n",
    "result_pd = spark.sql(\"\"\"\n",
    "    SELECT end_station_id, end_station_name\n",
    "    FROM sdf_view\n",
    "    WHERE end_station_id = '13285'\n",
    "\"\"\").toPandas()\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "print(result_pd.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a1f01c5a-d9fd-44ec-85ec-9812419a71fb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\nColumns: [end_station_id, end_station_name]\nIndex: []\n"
     ]
    }
   ],
   "source": [
    "# Test 3 - We know station id 'DIVVY CASSETTE REPAIR MOBILE STATION' previously existed. \n",
    "# The outcome of this test should return an empty dataframe.\n",
    "result_pd = spark.sql(\"\"\"\n",
    "    SELECT end_station_id, end_station_name\n",
    "    FROM sdf_view\n",
    "    WHERE end_station_id = 'DIVVY CASSETTE REPAIR MOBILE STATION'\n",
    "\"\"\").toPandas()\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "print(result_pd.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6967adb7-b1f9-476d-b574-9f3122736427",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 82:================================>                        (9 + 7) / 16]\r"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\nColumns: [start_station_id, start_station_name]\nIndex: []\n"
     ]
    }
   ],
   "source": [
    "# Test 4 - We had some rows with station id that contain the text 'DIVVY 001 - Warehouse test station' \n",
    "# They shouldn't exist anymore after we cleaned the data. The outcome of this test should return an empty dataframe.\n",
    "result_pd = spark.sql(\"\"\"\n",
    "    SELECT start_station_id, start_station_name\n",
    "    FROM sdf_view\n",
    "    WHERE start_station_id = 'DIVVY 001 - Warehouse test station'\n",
    "\"\"\").toPandas()\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "print(result_pd.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7836eec2-1f65-4b6b-aadc-915cefbb507f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# All good! Time to stop Spark\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "aws_spark_notebook",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (Spark High Memory)",
   "language": "python",
   "name": "spark_high_memory"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
