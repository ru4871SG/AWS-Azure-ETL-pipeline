{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "17ff0d85-15d2-424a-8268-3ad4fe598a8c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Import the necessary libraries\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.functions import regexp_replace\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7e08ef23-0dd6-4980-a273-1caef00565b9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Part 1. Get the data from AWS S3 Bucket and create a Spark DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "246072c3-bad4-4790-9fad-361e42e95fc5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# I've previously added the data from my S3 Bucket to Databricks Workspace. \n",
    "# Get the data directly and assign it to a Spark dataframe\n",
    "sdf = spark.table(\"de_testing_oregon.default.divvy_table_complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "723ae365-288b-43b4-afba-31db94088726",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Part 2. Preview the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5dde534e-8fd5-4473-97ab-2d0491e55cb1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- ride_id: string (nullable = true)\n |-- rideable_type: string (nullable = true)\n |-- started_at: timestamp (nullable = true)\n |-- ended_at: timestamp (nullable = true)\n |-- start_station_name: string (nullable = true)\n |-- start_station_id: string (nullable = true)\n |-- end_station_name: string (nullable = true)\n |-- end_station_id: string (nullable = true)\n |-- start_lat: double (nullable = true)\n |-- start_lng: double (nullable = true)\n |-- end_lat: double (nullable = true)\n |-- end_lng: double (nullable = true)\n |-- member_casual: string (nullable = true)\n |-- _rescued_data: string (nullable = true)\n\n5667717\n"
     ]
    }
   ],
   "source": [
    "# Print the schema\n",
    "sdf.printSchema()\n",
    "\n",
    "# Print the number of records\n",
    "print(sdf.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "70c279e2-4e72-4402-b76e-baf81e2e6dc1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- ride_id: string (nullable = true)\n |-- rideable_type: string (nullable = true)\n |-- started_at: timestamp (nullable = true)\n |-- ended_at: timestamp (nullable = true)\n |-- start_station_name: string (nullable = true)\n |-- start_station_id: string (nullable = true)\n |-- end_station_name: string (nullable = true)\n |-- end_station_id: string (nullable = true)\n |-- start_lat: double (nullable = true)\n |-- start_lng: double (nullable = true)\n |-- end_lat: double (nullable = true)\n |-- end_lng: double (nullable = true)\n |-- member_casual: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "# Databricks automatically added `_rescued_data`, it should be dropped in this notebook\n",
    "sdf = sdf.drop(\"_rescued_data\")\n",
    "\n",
    "sdf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "84389593-cc23-47a0-8eaa-bea99e045282",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+--------------+-------------------+-------------------+--------------------+----------------+--------------------+--------------+-------------+--------------+-----------------+------------------+-------------+\n|         ride_id| rideable_type|         started_at|           ended_at|  start_station_name|start_station_id|    end_station_name|end_station_id|    start_lat|     start_lng|          end_lat|           end_lng|member_casual|\n+----------------+--------------+-------------------+-------------------+--------------------+----------------+--------------------+--------------+-------------+--------------+-----------------+------------------+-------------+\n|C2F7DD78E82EC875|electric_bike |2022-01-13 11:59:47|2022-01-13 12:02:44|Glenwood Ave & To...|             525|Clark St & Touhy Ave|        RP-007|   42.0128005|    -87.665906|   42.01256011541|    -87.6743671152|       casual|\n|A6CF8980A652D272| electric_bike|2022-01-10 08:41:56|2022-01-10 08:46:17|Glenwood Ave & To...|             525|Clark St & Touhy Ave|        RP-007|    42.012763|   -87.6659675|   42.01256011541|    -87.6743671152|       casual|\n|BD0F91DFF741C66D|  classic_bike|2022-01-25 04:53:40|2022-01-25 04:58:01|Sheffield Ave & F...|    TA1306000016|Greenview Ave & F...|  TA1307000001|41.9256018819|-87.6537080423|         41.92533|          -87.6658|       member|\n|CBB80ED419105406|  classic_bike|2022-01-04 00:18:04|2022-01-04 00:33:00|Clark St & Bryn M...|    KA1504000151|Paulina St & Mont...|  TA1309000021|    41.983593|    -87.669154|        41.961507|        -87.671387|       casual|\n|DDC963BFDDA51EEA|  classic_bike|2022-01-20 01:31:10|2022-01-20 01:37:12|Michigan Ave & Ja...|    TA1309000002|State St & Randol...|  TA1305000029|     41.87785|     -87.62408|41.88462107257936|-87.62783423066139|       member|\n+----------------+--------------+-------------------+-------------------+--------------------+----------------+--------------------+--------------+-------------+--------------+-----------------+------------------+-------------+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "# Show the first 5 records\n",
    "sdf.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a491cf1a-3c7e-4ae6-acba-500625580329",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Part 3. General Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d6bb8781-06b4-4ca4-b133-cd144d648e2a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows with leading/trailing whitespace in the DataFrame:\n+----------------+-------------+-------------------+-------------------+---------------------------------------+----------------+---------------------------------------+--------------+------------------+------------------+------------------+------------------+-------------+----------------------+----------------------------+---------------------------------+-------------------------------+-------------------------------+-----------------------------+----------------------------+\n|ride_id         |rideable_type|started_at         |ended_at           |start_station_name                     |start_station_id|end_station_name                       |end_station_id|start_lat         |start_lng         |end_lat           |end_lng           |member_casual|ride_id_has_whitespace|rideable_type_has_whitespace|start_station_name_has_whitespace|start_station_id_has_whitespace|end_station_name_has_whitespace|end_station_id_has_whitespace|member_casual_has_whitespace|\n+----------------+-------------+-------------------+-------------------+---------------------------------------+----------------+---------------------------------------+--------------+------------------+------------------+------------------+------------------+-------------+----------------------+----------------------------+---------------------------------+-------------------------------+-------------------------------+-----------------------------+----------------------------+\n|E1F3D3583F47A9BD|electric_bike|2022-08-27 15:50:03|2022-08-27 15:56:47|Public Rack - Rockwell Ave & Touhy Ave |480             |Clark St & Touhy Ave                   |RP-007        |42.01             |-87.7             |42.01256011541    |-87.6743671152    |member       |false                 |false                       |true                             |false                          |false                          |false                        |false                       |\n|798AEBEEA62AB777|electric_bike|2022-08-13 18:10:08|2022-08-13 18:20:31|Public Rack - Rockwell Ave & Touhy Ave |480             |Western Ave & Lunt Ave                 |RP-005        |42.01             |-87.7             |42.0085943972     |-87.6904922389    |member       |false                 |false                       |true                             |false                          |false                          |false                        |false                       |\n|60651E4BEC5CFB33|electric_bike|2022-08-11 20:35:52|2022-08-11 20:42:27|Public Rack - Rockwell Ave & Touhy Ave |480             |Clark St & Lunt Ave                    |KA1504000162  |42.01             |-87.7             |42.009011180580764|-87.67411172389984|member       |false                 |false                       |true                             |false                          |false                          |false                        |false                       |\n|9E872277C7DC1293|electric_bike|2022-08-27 20:32:27|2022-08-27 20:47:56|Bosworth Ave & Howard St               |16806           |Public Rack - Rockwell Ave & Touhy Ave |480           |42.019551166666666|-87.66951483333334|42.01             |-87.7             |casual       |false                 |false                       |false                            |false                          |true                           |false                        |false                       |\n|B7AA1D6915C0673F|electric_bike|2022-08-25 17:23:23|2022-08-25 17:30:47|Bosworth Ave & Howard St               |16806           |Public Rack - Rockwell Ave & Touhy Ave |480           |42.019646333333334|-87.66944466666666|42.01             |-87.7             |casual       |false                 |false                       |false                            |false                          |true                           |false                        |false                       |\n|D2E6DA663EF3CFEB|electric_bike|2022-08-26 17:26:48|2022-08-26 17:33:57|Bosworth Ave & Howard St               |16806           |Public Rack - Rockwell Ave & Touhy Ave |480           |42.01947          |-87.66951116666667|42.01             |-87.7             |casual       |false                 |false                       |false                            |false                          |true                           |false                        |false                       |\n+----------------+-------------+-------------------+-------------------+---------------------------------------+----------------+---------------------------------------+--------------+------------------+------------------+------------------+------------------+-------------+----------------------+----------------------------+---------------------------------+-------------------------------+-------------------------------+-----------------------------+----------------------------+\nonly showing top 6 rows\n\nNumber of rows with whitespace in the DataFrame: 328\n"
     ]
    }
   ],
   "source": [
    "# Let's begin by checking trailing and leading whitespace\n",
    "def has_whitespace(col):\n",
    "    return (F.length(col) != F.length(F.trim(col)))\n",
    "\n",
    "whitespace_check = [F.when(has_whitespace(F.col(c)), F.lit(True)).otherwise(F.lit(False)).alias(f\"{c}_has_whitespace\") \n",
    "                    for c in sdf.columns if sdf.schema[c].dataType == StringType()]\n",
    "\n",
    "sdf_with_check = sdf.select(\"*\", *whitespace_check)\n",
    "rows_with_whitespace = sdf_with_check.filter(F.array_contains(F.array(*[f\"{c}_has_whitespace\" for c in sdf.columns if sdf.schema[c].dataType == StringType()]), True))\n",
    "\n",
    "print(\"Rows with leading/trailing whitespace in the DataFrame:\")\n",
    "rows_with_whitespace.show(6, truncate=False)\n",
    "\n",
    "# Count the rows with whitespace\n",
    "original_count = rows_with_whitespace.count()\n",
    "\n",
    "print(f\"Number of rows with whitespace in the DataFrame: {original_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7d257656-edcc-490b-9021-cc3c4fb406e3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Now we know that there are multiple rows with whitespace, we need to use trim() to fix this\n",
    "sdf = sdf.select([F.trim(F.col(c)).alias(c) if sdf.schema[c].dataType == StringType() else F.col(c) for c in sdf.columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "10a4b67a-5984-4309-9be7-180c6e681dcf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows with leading/trailing whitespace in the DataFrame:\n+-------+-------------+----------+--------+------------------+----------------+----------------+--------------+---------+---------+-------+-------+-------------+----------------------+----------------------------+---------------------------------+-------------------------------+-------------------------------+-----------------------------+----------------------------+\n|ride_id|rideable_type|started_at|ended_at|start_station_name|start_station_id|end_station_name|end_station_id|start_lat|start_lng|end_lat|end_lng|member_casual|ride_id_has_whitespace|rideable_type_has_whitespace|start_station_name_has_whitespace|start_station_id_has_whitespace|end_station_name_has_whitespace|end_station_id_has_whitespace|member_casual_has_whitespace|\n+-------+-------------+----------+--------+------------------+----------------+----------------+--------------+---------+---------+-------+-------+-------------+----------------------+----------------------------+---------------------------------+-------------------------------+-------------------------------+-----------------------------+----------------------------+\n+-------+-------------+----------+--------+------------------+----------------+----------------+--------------+---------+---------+-------+-------+-------------+----------------------+----------------------------+---------------------------------+-------------------------------+-------------------------------+-----------------------------+----------------------------+\n\nNumber of rows with whitespace in the DataFrame: 0\n"
     ]
    }
   ],
   "source": [
    "# Since we've cleaned the data, and since we've updated 'sdf', let's check the whitespace again\n",
    "sdf_with_check = sdf.select(\"*\", *whitespace_check)\n",
    "rows_with_whitespace = sdf_with_check.filter(F.array_contains(F.array(*[f\"{c}_has_whitespace\" for c in sdf.columns if sdf.schema[c].dataType == StringType()]), True))\n",
    "\n",
    "print(\"Rows with leading/trailing whitespace in the DataFrame:\")\n",
    "rows_with_whitespace.show(truncate=False)\n",
    "\n",
    "# Count the rows with whitespace in the original DataFrame\n",
    "original_count = rows_with_whitespace.count()\n",
    "\n",
    "print(f\"Number of rows with whitespace in the DataFrame: {original_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7ad48415-fd8a-4270-aab5-e51e7825d2e5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "For the next data cleaning steps, let's utilize Spark SQL. You can utilize Spark SQL or use standard DataFrame operations (up to you), but I utilize both of them in this notebook for demonstration purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2bbe71a8-72d7-4896-857f-10bc4df308a1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# To utilize Spark SQL, we can start by creating a temporary view\n",
    "sdf.createOrReplaceTempView(\"sdf_view\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c311c254-c03f-4f82-9142-d85b787238eb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------+----------+--------+------------------+----------------+----------------+--------------+---------+---------+-------+-------+-------------+\n|ride_id|rideable_type|started_at|ended_at|start_station_name|start_station_id|end_station_name|end_station_id|start_lat|start_lng|end_lat|end_lng|member_casual|\n+-------+-------------+----------+--------+------------------+----------------+----------------+--------------+---------+---------+-------+-------+-------------+\n|      0|            0|         0|       0|            833064|          833064|          892742|        892742|        0|        0|   5858|   5858|            0|\n+-------+-------------+----------+--------+------------------+----------------+----------------+--------------+---------+---------+-------+-------+-------------+\n\n"
     ]
    }
   ],
   "source": [
    "# Count the number of null values in each column\n",
    "null_counts_query = \"SELECT \" + \", \".join([f\"SUM(CASE WHEN {c} IS NULL THEN 1 ELSE 0 END) AS {c}\" for c in sdf.columns]) + \" FROM sdf_view\"\n",
    "null_counts = spark.sql(null_counts_query)\n",
    "null_counts.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0618215d-34ae-4395-ad03-e84c55313150",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4369360\n"
     ]
    }
   ],
   "source": [
    "# Now we know that there are null values, let's use Spark SQL to drop rows with any null values\n",
    "sdf = spark.sql(\"SELECT * FROM sdf_view WHERE \" + \" AND \".join([f\"{c} IS NOT NULL\" for c in sdf.columns]))\n",
    "\n",
    "# Show the number of records after dropping rows with null values\n",
    "print(sdf.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f8141f74-9c09-4f1e-86eb-f64db133348c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4369360\n"
     ]
    }
   ],
   "source": [
    "# Show the number of records after dropping rows with null values\n",
    "print(sdf.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4b7467d5-0ade-4fd0-8d8a-128282078fa5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Update the temporary view since we have updated the Spark DataFrame\n",
    "sdf.createOrReplaceTempView(\"sdf_view\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d1b3740b-14a4-4305-976b-e677eb002fcd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+-----------+\n|         started_at|           ended_at|ride_length|\n+-------------------+-------------------+-----------+\n|2022-10-13 14:42:10|2022-10-13 11:53:28|     -10122|\n|2022-06-07 19:14:47|2022-06-07 17:05:42|      -7745|\n|2022-06-07 19:14:46|2022-06-07 17:07:45|      -7621|\n|2022-11-06 01:58:11|2022-11-06 01:00:12|      -3479|\n|2022-11-06 01:59:05|2022-11-06 01:02:03|      -3422|\n|2022-11-06 01:57:21|2022-11-06 01:02:07|      -3314|\n+-------------------+-------------------+-----------+\nonly showing top 6 rows\n\n"
     ]
    }
   ],
   "source": [
    "# According to Divvy Data website, trips below 60 seconds in length should be removed. \n",
    "# Therefore, let's create \"ride_length\" based on \"started_at\" and \"ended_at\" to calculate and investigate this\n",
    "spark.sql(\"\"\"\n",
    "    SELECT *\n",
    "    FROM (\n",
    "        SELECT started_at, ended_at, (unix_timestamp(ended_at) - unix_timestamp(started_at)) AS ride_length \n",
    "        FROM sdf_view\n",
    "    ) subquery\n",
    "    WHERE ride_length < 60\n",
    "    ORDER BY ride_length\n",
    "\"\"\").show(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0f0cf2fe-8901-4f18-a583-b7e566f1df7d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-------------+-------------------+-------------------+--------------------+----------------+--------------------+--------------+------------------+------------------+---------+----------+-------------+-----------+\n|         ride_id|rideable_type|         started_at|           ended_at|  start_station_name|start_station_id|    end_station_name|end_station_id|         start_lat|         start_lng|  end_lat|   end_lng|member_casual|ride_length|\n+----------------+-------------+-------------------+-------------------+--------------------+----------------+--------------------+--------------+------------------+------------------+---------+----------+-------------+-----------+\n|EA062ADDDD68FFE9|electric_bike|2022-08-02 23:46:20|2022-08-02 23:47:20| Morgan St & Polk St|    TA1307000130| Morgan St & Polk St|  TA1307000130|41.871940333333335|-87.65101016666667|41.871737| -87.65103|       casual|         60|\n|599160B384DC8E6D| classic_bike|2022-01-04 09:46:35|2022-01-04 09:47:35|Wells St & Hubbar...|    TA1307000151|Orleans St & Hubb...|           636|         41.889906|        -87.634266|41.890028|-87.636618|       member|         60|\n|6A91F974716C863A|electric_bike|2022-09-03 20:24:01|2022-09-03 20:25:01|Broadway & Wilson...|           13074|Broadway & Wilson...|         13074| 41.96514966666667|-87.65844233333334|41.965221|-87.658139|       member|         60|\n|EBE8BAD30C77D81E| classic_bike|2022-01-12 12:26:08|2022-01-12 12:27:08|Clinton St & Madi...|    TA1305000032|Canal St & Madiso...|         13341|         41.882242|        -87.641066|41.882091|-87.639833|       member|         60|\n|6944ACDC4C4BB482|  docked_bike|2022-08-09 11:07:34|2022-08-09 11:08:34|Michigan Ave & Oa...|           13042|Michigan Ave & Oa...|         13042|          41.90096|        -87.623777| 41.90096|-87.623777|       casual|         60|\n|B4B0D398AF432821| classic_bike|2022-01-19 08:56:15|2022-01-19 08:57:15|Orleans St & Hubb...|             636|Wells St & Hubbar...|  TA1307000151|         41.890028|        -87.636618|41.889906|-87.634266|       member|         60|\n+----------------+-------------+-------------------+-------------------+--------------------+----------------+--------------------+--------------+------------------+------------------+---------+----------+-------------+-----------+\nonly showing top 6 rows\n\n"
     ]
    }
   ],
   "source": [
    "# Now we know that there are ride_length with less than 60 seconds (they even have minus numbers), it's time to clean them up\n",
    "sdf = spark.sql(\"\"\"\n",
    "        SELECT *\n",
    "        FROM (\n",
    "            SELECT *, (unix_timestamp(ended_at) - unix_timestamp(started_at)) AS ride_length \n",
    "            FROM sdf_view\n",
    "        ) subquery\n",
    "        WHERE ride_length >= 60\n",
    "        ORDER BY ride_length\n",
    "    \"\"\")\n",
    "\n",
    "sdf.show(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c55836f6-f174-4d5a-90eb-c4aa6670a61a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Update the temporary view since we have updated the Spark DataFrame\n",
    "sdf.createOrReplaceTempView(\"sdf_view\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "80c53d5b-4291-4740-9b83-70446ae0ebb7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Part 4. Investigate Data Quality Issues (before more specific data cleaning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "039e5c4c-1a0c-49c3-b245-414745dfc972",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-------------+-------------------+-------------------+------------------+--------------------+--------------------+--------------+------------------+------------------+------------------+------------------+-------------+-----------+\n|         ride_id|rideable_type|         started_at|           ended_at|start_station_name|    start_station_id|    end_station_name|end_station_id|         start_lat|         start_lng|           end_lat|           end_lng|member_casual|ride_length|\n+----------------+-------------+-------------------+-------------------+------------------+--------------------+--------------------+--------------+------------------+------------------+------------------+------------------+-------------+-----------+\n|0E2BDA1E247E7F0C|electric_bike|2022-08-13 21:06:10|2022-08-13 21:15:15|   WEST CHI-WATSON|DIVVY 001 - Wareh...|  Long & Irving Park|           398|41.927668833333335|-87.76981549999999|             41.95|            -87.76|       casual|        545|\n|A09927982BFB48F1|electric_bike|2022-04-23 18:43:39|2022-04-23 19:10:22|   WEST CHI-WATSON|           DIVVY 001|St Louis Ave & No...|           385|         41.794383|        -87.711208|             41.79|            -87.71|       casual|       1603|\n|599C6337A1F1635A|electric_bike|2022-05-07 19:21:15|2022-05-07 19:29:33|   WEST CHI-WATSON|DIVVY 001 - Wareh...|Wood St & Taylor ...|         13285|        41.8516735|-87.69061833333333|41.869265218438194|-87.67373085021973|       member|        498|\n|226015676A82060F|electric_bike|2022-06-22 15:08:17|2022-06-22 15:25:14|   WEST CHI-WATSON|DIVVY 001 - Wareh...|Milwaukee Ave & F...|           428|         41.924738|-87.70064133333334|             41.92|             -87.7|       casual|       1017|\n+----------------+-------------+-------------------+-------------------+------------------+--------------------+--------------------+--------------+------------------+------------------+------------------+------------------+-------------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "# Let's check and investigate potential data quality issues. We can start by checking uppercase values from start_station_name \n",
    "# to find naming inconsistencies.\n",
    "spark.sql(\"\"\"\n",
    "    SELECT *\n",
    "    FROM sdf_view\n",
    "    WHERE UPPER(start_station_name) = start_station_name\n",
    "    ORDER BY start_station_name\n",
    "\"\"\").show(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3932f293-7b64-49fe-a2b5-52c05ee0c245",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "I found something interesting above. Check the rows where start_station_name have the value \"WEST CHI-WATSON\". These same rows show the start_station_id as \"DIVVY\" something. Let's check further if all start_station_id that contain the text \"DIVVY\" are simple test stations or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f9948119-468c-4883-8d59-ec7c7a0d04b0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     start_station_id start_station_name\n0  DIVVY 001 - Warehouse test station    WEST CHI-WATSON\n1  DIVVY 001 - Warehouse test station    WEST CHI-WATSON\n2                           DIVVY 001    WEST CHI-WATSON\n3  DIVVY 001 - Warehouse test station    WEST CHI-WATSON\n4  DIVVY 001 - Warehouse test station            WestChi\n5  DIVVY 001 - Warehouse test station            WestChi\n"
     ]
    }
   ],
   "source": [
    "# let's use pandas so we can display the full station names for the start_station_name\n",
    "result_pd = spark.sql(\"\"\"\n",
    "    SELECT start_station_id, start_station_name\n",
    "    FROM sdf_view\n",
    "    WHERE start_station_id LIKE '%DIVVY%'\n",
    "    ORDER BY start_station_name\n",
    "\"\"\").toPandas()\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "print(result_pd.head(6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "67115308-9578-4b14-b7e3-4fe621943f80",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "From the latest finding above, one row has the value \"DIVVY 001\" for its start_station_id, while the other rows have the value \"DIVVY 001 - Warehouse test station\". While it's likely \"DIVVY 001\" is also a test station, we are not sure, so let's ignore it for now. Let's just make a note to clean up the rows later, where the start_station_id has the word \"test\", but not specifically \"DIVVY\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "62e86f81-4bcb-4379-bfbe-3da234c3f26f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-------------+-------------------+-------------------+--------------------+----------------+--------------------+--------------------+---------+----------+------------------+------------------+-------------+-----------+\n|         ride_id|rideable_type|         started_at|           ended_at|  start_station_name|start_station_id|    end_station_name|      end_station_id|start_lat| start_lng|           end_lat|           end_lng|member_casual|ride_length|\n+----------------+-------------+-------------------+-------------------+--------------------+----------------+--------------------+--------------------+---------+----------+------------------+------------------+-------------+-----------+\n|E5B1B30C08A03D7B| classic_bike|2022-06-04 15:28:21|2022-06-04 15:46:03|   Adler Planetarium|           13431|DIVVY CASSETTE RE...|DIVVY CASSETTE RE...|41.866095|-87.607267|41.895543469489944|-87.60292053222655|       casual|       1062|\n|C6E1B2965BCED6B6| classic_bike|2022-08-02 23:36:06|2022-08-02 23:42:49|Avers Ave & Belmo...|           15640|DIVVY CASSETTE RE...|DIVVY CASSETTE RE...|41.939408|-87.723574|41.895543469489944|-87.60292053222655|       casual|        403|\n|F25FE175AFAB5DD0| classic_bike|2022-08-03 07:36:02|2022-08-03 07:42:49| Canal St & Adams St|           13011|DIVVY CASSETTE RE...|DIVVY CASSETTE RE...|41.879255|-87.639904|41.895543469489944|-87.60292053222655|       member|        407|\n|5441EAF88B22331F| classic_bike|2022-08-03 17:56:34|2022-08-03 18:02:46|DuSable Lake Shor...|    TA1309000049|DIVVY CASSETTE RE...|DIVVY CASSETTE RE...|41.940775|-87.639192|41.895543469489944|-87.60292053222655|       member|        372|\n|104C043BDD8E9184| classic_bike|2022-08-03 07:09:31|2022-08-03 07:14:49|Logan Blvd & Elst...|    TA1308000031|DIVVY CASSETTE RE...|DIVVY CASSETTE RE...|41.929465|-87.684158|41.895543469489944|-87.60292053222655|       member|        318|\n|685D6988F1C63EF6| classic_bike|2022-08-03 17:37:08|2022-08-03 17:40:46|Southport Ave & I...|    TA1309000043|DIVVY CASSETTE RE...|DIVVY CASSETTE RE...|41.954177|-87.664358|41.895543469489944|-87.60292053222655|       casual|        218|\n+----------------+-------------+-------------------+-------------------+--------------------+----------------+--------------------+--------------------+---------+----------+------------------+------------------+-------------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "# Next, let's check the uppercase values from end_station_name\n",
    "spark.sql(\"\"\"\n",
    "    SELECT *\n",
    "    FROM sdf_view\n",
    "    WHERE UPPER(end_station_name) = end_station_name\n",
    "    ORDER BY start_station_name\n",
    "\"\"\").show(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "adcb6133-9d9f-4eab-a872-1357fa34b6a3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Another interesting finding here. The above result shows that for the rows that contain the text \"DIVVY\" in their end_station_name, they also start with the text \"DIVVY\" for their end_station_id. Let's investigate this further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9380b138-cfb8-4dff-9593-db1e75c7e471",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         end_station_id                      end_station_name\n0  DIVVY CASSETTE REPAIR MOBILE STATION  DIVVY CASSETTE REPAIR MOBILE STATION\n1  DIVVY CASSETTE REPAIR MOBILE STATION  DIVVY CASSETTE REPAIR MOBILE STATION\n2  DIVVY CASSETTE REPAIR MOBILE STATION  DIVVY CASSETTE REPAIR MOBILE STATION\n3  DIVVY CASSETTE REPAIR MOBILE STATION  DIVVY CASSETTE REPAIR MOBILE STATION\n4  DIVVY CASSETTE REPAIR MOBILE STATION  DIVVY CASSETTE REPAIR MOBILE STATION\n5  DIVVY CASSETTE REPAIR MOBILE STATION  DIVVY CASSETTE REPAIR MOBILE STATION\n"
     ]
    }
   ],
   "source": [
    "# Just like before, let's use pandas so we can display the full station names (but this time for the end_station_name)\n",
    "result_pd = spark.sql(\"\"\"\n",
    "    SELECT end_station_id, end_station_name\n",
    "    FROM sdf_view\n",
    "    WHERE end_station_id LIKE '%DIVVY%' OR end_station_name LIKE '%DIVVY%'\n",
    "    ORDER BY end_station_name\n",
    "\"\"\").toPandas()\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "print(result_pd.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b871b360-fc57-410b-a38a-82678ec928c5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "There are rows with value \"DIVVY CASSETTE REPAIR MOBILE STATION\", which means they are just used for maintenance, not for actual trips. This needs to be filtered as well later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "176c477c-5498-43b9-9d35-e9072e3792d8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------+----------+--------+------------------+----------------+----------------+--------------+---------+---------+-------+-------+-------------+-----------+\n|ride_id|rideable_type|started_at|ended_at|start_station_name|start_station_id|end_station_name|end_station_id|start_lat|start_lng|end_lat|end_lng|member_casual|ride_length|\n+-------+-------------+----------+--------+------------------+----------------+----------------+--------------+---------+---------+-------+-------+-------------+-----------+\n+-------+-------------+----------+--------+------------------+----------------+----------------+--------------+---------+---------+-------+-------+-------------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "# Now, let's check the lowercase values from start_station_name\n",
    "spark.sql(\"\"\"\n",
    "    SELECT *\n",
    "    FROM sdf_view\n",
    "    WHERE LOWER(start_station_name) = start_station_name\n",
    "    ORDER BY start_station_name\n",
    "\"\"\").show(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "342c94c3-1bd6-4914-b0fc-10d6497208d0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "There's nothing here, let's continue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "286ab3ae-6194-47d7-b2a3-51d8e25e6f02",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------+----------+--------+------------------+----------------+----------------+--------------+---------+---------+-------+-------+-------------+-----------+\n|ride_id|rideable_type|started_at|ended_at|start_station_name|start_station_id|end_station_name|end_station_id|start_lat|start_lng|end_lat|end_lng|member_casual|ride_length|\n+-------+-------------+----------+--------+------------------+----------------+----------------+--------------+---------+---------+-------+-------+-------------+-----------+\n+-------+-------------+----------+--------+------------------+----------------+----------------+--------------+---------+---------+-------+-------+-------------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "# Let's check the lowercase values from end_station_name\n",
    "spark.sql(\"\"\"\n",
    "    SELECT *\n",
    "    FROM sdf_view\n",
    "    WHERE LOWER(end_station_name) = end_station_name\n",
    "    ORDER BY end_station_name\n",
    "\"\"\").show(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "59274575-69c9-4c0a-b51b-0491b91c2728",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Nothing to see here as well. Let's continue. \n",
    "\n",
    "Now that we know some rows have the word \"test\" in start_station_id (from one of the previous investigations), we should check all the station columns for the word \"test\". Let's check regardless of their case sensitivity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8600300d-96ea-4857-bb67-60efb7f738d3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      start_station_id  ...            end_station_name\n0  Hubbard Bike-checking (LBS-WH-TEST)  ...       Wood St & Webster Ave\n1  Hubbard Bike-checking (LBS-WH-TEST)  ...  Wood St & Taylor St (Temp)\n2   DIVVY 001 - Warehouse test station  ...  Wood St & Taylor St (Temp)\n3  Hubbard Bike-checking (LBS-WH-TEST)  ...     Wood St & Milwaukee Ave\n4  Hubbard Bike-checking (LBS-WH-TEST)  ...     Wood St & Milwaukee Ave\n5  Hubbard Bike-checking (LBS-WH-TEST)  ...     Wood St & Milwaukee Ave\n6  Hubbard Bike-checking (LBS-WH-TEST)  ...     Wood St & Milwaukee Ave\n7  Hubbard Bike-checking (LBS-WH-TEST)  ...        Wood St & Hubbard St\n8  Hubbard Bike-checking (LBS-WH-TEST)  ...        Wood St & Hubbard St\n9  Hubbard Bike-checking (LBS-WH-TEST)  ...       Wood St & Chicago Ave\n\n[10 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "# Again, let's use pandas so we can display the full station names\n",
    "result_pd = spark.sql(\"\"\"\n",
    "    SELECT start_station_id, start_station_name, end_station_id, end_station_name\n",
    "    FROM sdf_view\n",
    "    WHERE LOWER(start_station_name) LIKE '%test%'\n",
    "        OR LOWER(end_station_name) LIKE '%test%'\n",
    "        OR LOWER(start_station_id) LIKE '%test%'\n",
    "        OR LOWER(end_station_id) LIKE '%test%'\n",
    "    ORDER BY end_station_name DESC\n",
    "\"\"\").toPandas()\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "print(result_pd.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "13d04091-e26b-4525-afd2-7d1da31e0e61",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Apart from the issues with the word \"test\", there are also station names that end with \"(Temp)\", which probably means \"Temporary\". We will have to fix the naming inconsistencies here later. Let's investigate this further. Let's check for the word \"temp\" in the station columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ffd81eaf-5291-4744-b280-efbed74d45a9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  start_station_id  ...                    end_station_name\n0     KA1504000155  ...  California Ave & Francis Pl (Temp)\n1            15491  ...      Wentworth Ave & 24th St (Temp)\n2            13028  ...          Wood St & Taylor St (Temp)\n3            13028  ...          Wood St & Taylor St (Temp)\n4            13028  ...          Wood St & Taylor St (Temp)\n5            13028  ...          Wood St & Taylor St (Temp)\n\n[6 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "# Let's use pandas so we can check the full station names\n",
    "result_pd = spark.sql(\"\"\"\n",
    "    SELECT start_station_id, start_station_name, end_station_id, end_station_name\n",
    "    FROM sdf_view\n",
    "    WHERE LOWER(start_station_name) LIKE '%temp%'\n",
    "        OR LOWER(end_station_name) LIKE '%temp%'\n",
    "        OR LOWER(start_station_id) LIKE '%temp%'\n",
    "        OR LOWER(end_station_id) LIKE '%temp%'\n",
    "    ORDER BY start_station_name\n",
    "\"\"\").toPandas()\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "print(result_pd.head(6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d5be2ad8-dc21-40eb-b211-38e15a2bb2e2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "I found multiple station names with \"(Temp)\", we will have to fix the station names later. Another potential data quality issue is the presence of the asterisk character in the station columns. Let's check for the presence of the asterisk character in the station columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5f1b0f62-4461-4eb5-b646-6cec1da17a4a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  start_station_id  ...               end_station_name\n0     chargingstx3  ...          Green St & Madison St\n1     chargingstx3  ...        Green St & Randolph St*\n2     chargingstx3  ...  Sangamon St & Washington Blvd\n3     chargingstx3  ...     Green St & Washington Blvd\n4     chargingstx4  ...         N Green St & W Lake St\n5     TA1306000014  ...    Wilton Ave & Diversey Pkwy*\n\n[6 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "# Again, let's use pandas to check the full station names. This time, to check the names that contain asterisks (*)\n",
    "result_pd = spark.sql(\"\"\"\n",
    "    SELECT start_station_id, start_station_name, end_station_id, end_station_name\n",
    "    FROM sdf_view\n",
    "    WHERE LOWER(start_station_name) LIKE '%*%'\n",
    "        OR LOWER(end_station_name) LIKE '%*%'\n",
    "        OR LOWER(start_station_id) LIKE '%*%'\n",
    "        OR LOWER(end_station_id) LIKE '%*%'\n",
    "\"\"\").toPandas()\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "print(result_pd.head(6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d743bfc1-e5e8-4d12-9127-6944c2c10f7c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Many rows have asterisks in the station names, we need to fix them later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9ab9a158-6279-4720-94c6-c157d9ab5d39",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Part 5 - Specific Data Cleaning (to fix the issues from part 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "feafb4f1-3cff-4166-a901-49db67db8bcc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Clean and fix all the data quality issues we have identified from part 4\n",
    "sdf = spark.sql(\"\"\"\n",
    "    SELECT * \n",
    "    FROM sdf_view\n",
    "    WHERE upper(start_station_name) != start_station_name\n",
    "      AND upper(end_station_name) != end_station_name\n",
    "      AND lower(start_station_name) NOT LIKE '%test%'\n",
    "      AND lower(end_station_name) NOT LIKE '%test%'\n",
    "      AND lower(start_station_id) NOT LIKE '%test%'\n",
    "      AND lower(end_station_id) NOT LIKE '%test%'\n",
    "      AND lower(start_station_name) NOT LIKE 'divvy cassette repair mobile station'\n",
    "      AND lower(end_station_name) NOT LIKE 'divvy cassette repair mobile station'\n",
    "      AND lower(start_station_id) NOT LIKE 'divvy cassette repair mobile station'\n",
    "      AND lower(end_station_id) NOT LIKE 'divvy cassette repair mobile station'\n",
    "\"\"\")\n",
    "\n",
    "sdf = sdf.withColumn(\"start_station_name\", regexp_replace(\"start_station_name\", \"\\\\s?\\\\*\", \"\")) \\\n",
    "        .withColumn(\"start_station_name\", regexp_replace(\"start_station_name\", \"\\\\s?\\\\(Temp\\\\)\", \"\")) \\\n",
    "        .withColumn(\"end_station_name\", regexp_replace(\"end_station_name\", \"\\\\s?\\\\*\", \"\")) \\\n",
    "        .withColumn(\"end_station_name\", regexp_replace(\"end_station_name\", \"\\\\s?\\\\(Temp\\\\)\", \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3194ba8a-07bf-4837-9bae-cdde0a12c4d5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-------------+-------------------+-------------------+--------------------+----------------+--------------------+--------------+-----------------+------------------+-----------------+------------------+-------------+-----------+\n|         ride_id|rideable_type|         started_at|           ended_at|  start_station_name|start_station_id|    end_station_name|end_station_id|        start_lat|         start_lng|          end_lat|           end_lng|member_casual|ride_length|\n+----------------+-------------+-------------------+-------------------+--------------------+----------------+--------------------+--------------+-----------------+------------------+-----------------+------------------+-------------+-----------+\n|835B7FEAE64C26A3| classic_bike|2022-08-07 18:17:33|2022-08-07 18:18:33|Michigan Ave & Oa...|           13042|Michigan Ave & Oa...|         13042|      41.90096039|      -87.62377664|      41.90096039|      -87.62377664|       member|         60|\n|599160B384DC8E6D| classic_bike|2022-01-04 09:46:35|2022-01-04 09:47:35|Wells St & Hubbar...|    TA1307000151|Orleans St & Hubb...|           636|        41.889906|        -87.634266|        41.890028|        -87.636618|       member|         60|\n|6A91F974716C863A|electric_bike|2022-09-03 20:24:01|2022-09-03 20:25:01|Broadway & Wilson...|           13074|Broadway & Wilson...|         13074|41.96514966666667|-87.65844233333334|        41.965221|        -87.658139|       member|         60|\n|7CD5AEF6A8F819E2| classic_bike|2022-01-12 18:10:03|2022-01-12 18:11:03|Franklin St & Jac...|    TA1305000025|Franklin St & Ada...|  TA1309000008|    41.8777079559|    -87.6353211408|41.87943409140013|-87.63550400733948|       member|         60|\n|D5CBFD4D8BA6DCB0|electric_bike|2022-08-18 08:35:15|2022-08-18 08:36:15|Canal St & Madiso...|           13341|Canal St & Monroe St|         13056|     41.882101417|     -87.639317989|         41.88169|         -87.63953|       member|         60|\n+----------------+-------------+-------------------+-------------------+--------------------+----------------+--------------------+--------------+-----------------+------------------+-----------------+------------------+-------------+-----------+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "# Update the temporary view since we have updated the Spark DataFrame\n",
    "sdf.createOrReplaceTempView(\"sdf_view\")\n",
    "\n",
    "# Also, show the cleaned data\n",
    "sdf.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "54254696-b1c5-4922-8f71-f8b82af0c3c0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  start_station_id      start_station_name\n0     chargingstx3  Green St & Randolph St\n1     chargingstx3  Green St & Randolph St\n2     chargingstx3  Green St & Randolph St\n"
     ]
    }
   ],
   "source": [
    "# Test 1 - Making sure that the data has been properly cleaned, we should check the new 'sdf_view'. \n",
    "# We know station id 'chargingstx3' previously  had station name of 'Green St & Randolph St*', \n",
    "# so now we should see 'Green St & Randolph St' instead. \n",
    "result_pd = spark.sql(\"\"\"\n",
    "    SELECT start_station_id, start_station_name\n",
    "    FROM sdf_view\n",
    "    WHERE start_station_id = 'chargingstx3'\n",
    "\"\"\").toPandas()\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "print(result_pd.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8ff28300-e424-4b69-a355-72fa76aec9bf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  end_station_id     end_station_name\n0          13285  Wood St & Taylor St\n1          13285  Wood St & Taylor St\n2          13285  Wood St & Taylor St\n"
     ]
    }
   ],
   "source": [
    "# Test 2 - We know station id '13285' previously had station name of 'Wood St & Taylor St (Temp)', \n",
    "# so now we should see 'Wood St & Taylor St' instead.\n",
    "result_pd = spark.sql(\"\"\"\n",
    "    SELECT end_station_id, end_station_name\n",
    "    FROM sdf_view\n",
    "    WHERE end_station_id = '13285'\n",
    "\"\"\").toPandas()\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "print(result_pd.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a1f01c5a-d9fd-44ec-85ec-9812419a71fb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\nColumns: [end_station_id, end_station_name]\nIndex: []\n"
     ]
    }
   ],
   "source": [
    "# Test 3 - We know station id 'DIVVY CASSETTE REPAIR MOBILE STATION' previously existed. \n",
    "# The outcome of this test should return an empty dataframe.\n",
    "result_pd = spark.sql(\"\"\"\n",
    "    SELECT end_station_id, end_station_name\n",
    "    FROM sdf_view\n",
    "    WHERE end_station_id = 'DIVVY CASSETTE REPAIR MOBILE STATION'\n",
    "\"\"\").toPandas()\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "print(result_pd.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6967adb7-b1f9-476d-b574-9f3122736427",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\nColumns: [start_station_id, start_station_name]\nIndex: []\n"
     ]
    }
   ],
   "source": [
    "# Test 4 - We had some rows with station id that contain the text 'DIVVY 001 - Warehouse test station' \n",
    "# They shouldn't exist anymore after we cleaned the data. The outcome of this test should return an empty dataframe.\n",
    "result_pd = spark.sql(\"\"\"\n",
    "    SELECT start_station_id, start_station_name\n",
    "    FROM sdf_view\n",
    "    WHERE start_station_id = 'DIVVY 001 - Warehouse test station'\n",
    "\"\"\").toPandas()\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "print(result_pd.head(3))"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "aws_databricks_notebook",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (Spark High Memory)",
   "language": "python",
   "name": "spark_high_memory"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
